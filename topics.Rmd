% Topic Analysis of the ESA 2014 Program
% Noam Ross
% 14-08-09 17:15:24

In [my first pass at text analysis of the ESA program] I looked at how the frequency
of words used in the ESA program differed fram last year to this year.  But
there are much more sophisticated at looking at word use in text.
There are m
In this document, we fit an <a href='http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation' target='_blank'>LDA topic model</a> to the <a href='http://www.cs.cornell.edu/people/pabo/movie-review-data/' target='_blank'>movie review polarity dataset</a> shared by Pang, Lee, and Vaithyanathan (EMNLP, 2002, where we use 'polarity dataset version 2.0'). To fit the model, we used the R package <a href='http://cran.r-project.org/web/packages/mallet/' target='_blank'>mallet</a> and we visualize the output using <a href='https://github.com/cpsievert/LDAvis' target='_blank'>LDAvis</a>.

```{r setup, echo = FALSE, message = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE)
```

### Movie review data

We created an R package, <a href='https://github.com/cpsievert/moviereviews' target='_blank'>moviereviews</a>, to provide the movie reviews data as an R object so that it's easy to load and start an analysis. The object `reviews` is a list of character vectors where each character vector corresponds to a review. The elements of a given character vector corresponds to sentences in that particular review. Therefore, we can collapse these character vectors so that each review is contained in a single string.

```{r read}
library(plyr)
library(XML)
library(stringi)
ppath = "eco.confex.com/eco/2014/webprogram"
paper_files = list.files(ppath, recursive=TRUE, pattern="Paper\\d+\\.html")

abstracts = alply(paper_files, 1, function(paper) {
  paper_xml = htmlTreeParse(file.path(ppath, paper), useInternalNodes = TRUE, trim=TRUE)
  ab = try(xmlValue(paper_xml[['//div[@class="abstract"]']]), silent = TRUE)
  if(class(ab) != "try-error") {
  ab = stringi::stri_replace_all_fixed(ab, "Background/Question/Methods", "")
  ab = stringi::stri_replace_all_fixed(ab, "Results/Conclusions", "")
  return(ab)
  } else {
    return(NULL)
  }
}, .progress = "time")
abstracts = unlist(compact(abstracts))
```

### Pre-processing

Anytime we fit a topic model, it's a good idea to do some pre-processing (cleaning of text). This dataset is already fairly clean, so we only remove punctuation and some common <a href='http://en.wikipedia.org/wiki/Stop_words' target='_blank'>stop words</a>. In particular, we use the english stop words from the <a href='http://en.wikipedia.org/wiki/SMART_Information_Retrieval_System' target= '_blank'>SMART information retrieval system</a>.

```{r collect_stops}
download.file("http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop", "stopwords.txt")
```

### Using mallet for model fitting

The R package mallet provides an interface to the java-based <a href='http://mallet.cs.umass.edu/' target='_blank'>MAchine Learning for LanguagE Toolkit</a>. To get started, we need to import our reviews along with our stop words and document names into mallet.

```{r import}
library(mallet)
instance <- mallet.import(names(abstracts), abstracts, "stopwords.txt")
```

Next, we initiate a topic model with 25 topics (a reasonable number of topics for a data set of this size, in our experience), load documents into that model instance amd extract the frequency of words that appears in the corpus.

```{r freqs}
model <- MalletLDA(num.topics = 25)
model$loadDocuments(instance)
freqs <- mallet.word.freqs(model)
```

It's generally a good idea to remove really infrequent words from the model's vocabulary before actually training the model. In this case, we add terms that appear less than 10 times to our list of stopwords and re-import the model instance.

```{r stops}
stopwords <- as.character(subset(freqs, term.freq <= 9)$words)
# 's' and 't' show up frequently and aren't very informative, so they are also included as stopwords
writeLines(c(readLines("stopwords.txt"), stopwords, "s", "t"),  "stopwords2.txt")
# Re-'initiate' topic model without the infrequent words
instance2 <- mallet.import(names(abstracts), abstracts, "stopwords2.txt")
model2 <- MalletLDA(num.topics = 25)
model2$loadDocuments(instance2)
freqs2 <- mallet.word.freqs(model2)
```

### Model fitting/training

Now we're in position to use mallet's train method to fit the topic model. From the fitted model, we'll need a number of things to serve our visualization. Most important of all is the entire $\phi$ matrix where each column corresponds to a probability mass function over terms.

```{r fit}
# this takes about 4.5 minutes on a macbook pro laptop with 2GB RAM and a 2.26GHz processor 
model2$train(2000)
# Here, we compute the estimated topic-term distribution, incorporating the effect
# of the prior using 'smoothed = TRUE'.
phi <- t(mallet.topic.words(model2, smoothed = TRUE, normalized = TRUE))
# Let's look at the table of topics and terms by setting 'normalized = FALSE'
phi.count <- t(mallet.topic.words(model2, smoothed = TRUE, normalized = FALSE))
# Now get the smoothed estimates of the document-topic distributions:
topic.words <- mallet.topic.words(model2, smoothed = TRUE, normalized = FALSE)
# 'count' of the number of tokens per topic (including pseudo-tokens from the priors)
topic.counts <- rowSums(topic.words)
topic.proportions <- topic.counts/sum(topic.counts)
vocab <- model2$getVocabulary()
```

### LDAvis

LDAvis comes with a `check.inputs` function that ensures visualization inputs are valid. In addition, `check.inputs` also reorders the columns of the $\phi$ matrix according to the `topic.proportions`. Here we label these columns so that '1' stands for the topic that is responsible for the highest share of the corpus.

```{r vis, results='hide'}
# LDAvis can be installed from GitHub via `devtools::install_github("cpsievert/LDAvis")`
library(LDAvis)
out <- check.inputs(K = 25, W = length(vocab), phi = phi, 
                    term.frequency = apply(phi.count, 1, sum), 
                    vocab = vocab, topic.proportion = topic.proportions)
# Relabel topics so that topics are numbered in decreasing order of frequency.
colnames(out$phi) <- seq_len(out$K)
```

Now that the "inputs" look OK, `createJSON` converts them into a JSON object that is used to drive the interactive visualization.

```{r createJSON, results='hide'}
json <- with(out, createJSON(K = 25, phi, term.frequency, 
                   vocab, topic.proportion))
```

Lastly, the `serVis` function will write a number of files including index.html which can be viewed in a web browser or embeded within other pages via an HTML `<iframe>`. See [here](http://www2.research.att.com/~kshirley/lda/index.html) for more info regarding reading the using the visualization. 

```{r serVis}
serVis(json, out.dir = 'vis', open.browser = TRUE)
```

<iframe src = "vis/index.html" width = "1200" height = "700"></iframe> 

